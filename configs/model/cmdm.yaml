name: CMDM

## modeling space
input_feats: -1
data_repr: 'pos'

## time embedding
time_emb_dim: 512

## conditions
## 1. contact
contact_model:
  contact_type: 'contact_cont_joints' # ['contact_one_joints', 'contact_all_joints', 'contact_cont_joints', 'contact_pelvis']
  contact_joints: [0, 10, 11, 12, 20, 21]

  planes: [32, 64, 128, 256]
  num_points: ${task.dataset.num_points}
  blocks: [2, 2, 2, 2]

## 2. text
text_model:
  version: 'ViT-B/32'
  max_length: 32

### model architecture
## trans_mamba 配置（原始配置，已注释）
#arch: 'trans_mamba'
#latent_dim: 512
#mask_motion: true
#num_layers: [1,1,1,1,1]
#num_heads: 8
#dropout: 0.1
#dim_feedforward: 1024

## trans_enc 配置（原始配置，已注释）
#arch: 'trans_enc'
#latent_dim: 512
#mask_motion: true
#num_layers: [1,1,1,1,1]
#num_heads: 8
#dropout: 0.1
#dim_feedforward: 1024

# 模型架构选择 - DiT 稳定配置
arch: "dit"

# DiT 核心参数
latent_dim: 512
mask_motion: true

# 层数配置 (3个阶段，每阶段8层，共24层)
num_layers: [8, 8, 8]

# 注意力头数
num_heads: 8

# MLP 扩展倍数
dim_feedforward: 2048

# Dropout (增加正则化)
dropout: 0.15

# DiT 专用参数
dit_drop_path: 0.1
dit_use_cross_attn_pooling: true

# 条件嵌入配置
condition_embedder:
  use_cross_attn_pooling: true
  num_latents: 128
  fusion_method: 'cross_attn'

# 训练稳定性参数
grad_clip: 1.0
weight_decay: 1e-4
warmup_steps: 2000
lr_scheduler: 'cosine'

# =============================================================================
# DiT (Diffusion Transformer) 架构配置
# =============================================================================
# 使用方法: 将 arch 改为 'dit' 或 'dit_cross'
#
# arch: 'dit'           # 纯DiT: 使用AdaLN注入条件，条件通过cross-attn pooling融合
# arch: 'dit_cross'     # DiT+Cross: AdaLN + 每层对contact做cross-attention
#
# DiT 特有参数:
# dit_drop_path: 0.1              # Stochastic Depth rate (线性增长)
# dit_use_cross_attn_pooling: true  # 用cross-attention聚合contact特征 (仅dit)
#
# 推荐配置 (HumanML3D):
# arch: 'dit'
# latent_dim: 512
# num_layers: [1,1,1,1,1,1,1,1]   # 总共8层DiT blocks
# num_heads: 8
# dim_feedforward: 2048           # MLP ratio = 4
# dropout: 0.1
# dit_drop_path: 0.1
# =============================================================================