import os
import hydra
import torch
import random
import numpy as np
from loguru import logger
from omegaconf import DictConfig, OmegaConf
from torch.utils.data.distributed import DistributedSampler

from datasets.base import create_dataset
from datasets.misc import collate_fn_general
from models.base import create_model_and_diffusion
from utils.io import mkdir_if_not_exists, Board
from utils.training import TrainLoop
from utils.misc import compute_repr_dimesion


def print_model_architecture(model):
    """
    å¯è§†åŒ–æ‰“å°æ¨¡å‹æ¯ä¸€å±‚çš„ç±»å‹ (Transformer vs Mamba)
    """
    logger.info("\n" + "=" * 60)
    logger.info("       [Model Architecture Layout Check]       ")
    logger.info("=" * 60)

    # å¤„ç† DDP åŒ…è£¹çš„æƒ…å†µ (è™½ç„¶é€šå¸¸æˆ‘ä»¬åœ¨ DDP ä¹‹å‰è°ƒç”¨)
    real_model = model.module if hasattr(model, 'module') else model

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ encoder_layers
    if hasattr(real_model, 'encoder_layers'):
        layers = real_model.encoder_layers

        for i, layer in enumerate(layers):
            layer_class = type(layer).__name__

            # æ ¹æ®ç±»åå®šä¹‰æ ‡ç­¾
            if "Mamba" in layer_class:
                tag = "ğŸ [MAMBA]"
                desc = f"Layer {i}: {layer_class} (Param Reset/Finetune)"
            elif "Transformer" in layer_class:
                tag = "ğŸ¤– [TRANS]"
                desc = f"Layer {i}: {layer_class} (Pre-trained)"
            else:
                tag = "â“ [OTHER]"
                desc = f"Layer {i}: {layer_class}"

            logger.info(f"{tag:<10} | {desc}")

    else:
        logger.warning("Model does not have 'encoder_layers' attribute.")

    logger.info("=" * 60 + "\n")

@hydra.main(version_base=None, config_path="./configs", config_name="default")
def main(cfg: DictConfig) -> None:
    # ... (å‰é¢çš„åˆå§‹åŒ–ä»£ç ä¿æŒä¸å˜) ...
    cfg.model.input_feats = compute_repr_dimesion(cfg.model.data_repr)
    cfg.gpu = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(cfg.gpu)
    device = torch.device('cuda', cfg.gpu)
    torch.distributed.init_process_group(backend='nccl')

    if cfg.gpu == 0:
        logger.remove(handler_id=0)
        mkdir_if_not_exists(cfg.log_dir)
        mkdir_if_not_exists(cfg.ckpt_dir)
        mkdir_if_not_exists(cfg.eval_dir)
        logger.add(cfg.log_dir + '/runtime.log')
        Board().create_board(cfg.platform, project=cfg.project, log_dir=cfg.log_dir)
        logger.info('[Configuration]\n' + OmegaConf.to_yaml(cfg) + '\n')

    # ... (Dataset å’Œ Dataloader éƒ¨åˆ†ä¿æŒä¸å˜) ...
    train_dataset = create_dataset(cfg.task.dataset, cfg.task.train.phase, gpu=cfg.gpu)
    train_sampler = DistributedSampler(train_dataset, shuffle=True)
    train_dataloader = train_dataset.get_dataloader(
        sampler=train_sampler,
        batch_size=cfg.task.train.batch_size,
        collate_fn=collate_fn_general,
        num_workers=cfg.task.train.num_workers,
        pin_memory=True,
    )

    ## create model
    model, diffusion = create_model_and_diffusion(cfg, device=device)
    model.to(device)

    # ================= [æ–°å¢] æ‰“å°æ¶æ„ä¿¡æ¯ =================
    if cfg.gpu == 0:
        print_model_architecture(model)
    # =======================================================

    # ==============================================================================
    # [å…³é”®ä¿®æ”¹] æ‰‹åŠ¨å¤„ç†æƒé‡åŠ è½½ (Transfer Learning)
    # ==============================================================================
    resume_path = cfg.task.train.get('resume_ckpt', None)

    # åªæœ‰å½“è·¯å¾„å­˜åœ¨æ—¶æ‰æ‰§è¡Œè‡ªå®šä¹‰åŠ è½½
    if resume_path and os.path.exists(resume_path):
        if cfg.gpu == 0:
            logger.info(f"[Transfer] Found checkpoint: {resume_path}")
            logger.info(f"[Transfer] Starting partial weight loading...")

        # 1. åŠ è½½ Checkpoint
        checkpoint = torch.load(resume_path, map_location=device)
        if 'model' in checkpoint:
            pretrained_dict = checkpoint['model']
        elif 'state_dict' in checkpoint:
            pretrained_dict = checkpoint['state_dict']
        else:
            pretrained_dict = checkpoint

        model_dict = model.state_dict()
        new_state_dict = {}

        # 2. å®šä¹‰è¦ä¿ç•™å’Œè·³è¿‡çš„å±‚
        # ä½ çš„éœ€æ±‚ï¼šä¿ç•™ Layer 0, 1 (Transformer)ï¼›è·³è¿‡ Layer 2, 3, 4 (Mamba)
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ çš„æ¨¡å‹å±‚å‘½åæ˜¯ 'encoder_layers.0', 'encoder_layers.1' ç­‰
        layers_to_skip = ["encoder_layers.2", "encoder_layers.3", "encoder_layers.4"]

        for k, v in pretrained_dict.items():
            k_clean = k.replace("module.", "")  # å»æ‰ DDP å‰ç¼€

            # æ£€æŸ¥æ˜¯å¦åœ¨è·³è¿‡åˆ—è¡¨ä¸­
            is_skipped = False
            for skip_str in layers_to_skip:
                if skip_str in k_clean:
                    is_skipped = True
                    break

            if is_skipped:
                continue  # è·³è¿‡è¯¥å±‚æƒé‡

            # æ£€æŸ¥æ˜¯å¦åœ¨æ–°æ¨¡å‹ä¸­å­˜åœ¨ä¸”å½¢çŠ¶ä¸€è‡´
            if k_clean in model_dict:
                if v.shape == model_dict[k_clean].shape:
                    new_state_dict[k_clean] = v
                else:
                    if cfg.gpu == 0:
                        logger.warning(f"[Transfer] Shape mismatch ignored: {k_clean}")

        # 3. åŠ è½½æƒé‡ (strict=False)
        model.load_state_dict(new_state_dict, strict=False)

        if cfg.gpu == 0:
            logger.info(f"[Transfer] Loaded {len(new_state_dict)} keys.")
            logger.info(f"[Transfer] Layers 0 & 1 loaded from Transformer.")
            logger.info(f"[Transfer] Layers 2, 3, 4 (Mamba) initialized from scratch.")

        # 4. [æœ€é‡è¦çš„ä¸€æ­¥] æ¸…ç©º resume_ckpt
        # è¿™ä¼šå‘Šè¯‰ TrainLoopï¼šä¸è¦åŠ è½½ä¼˜åŒ–å™¨ï¼Œåˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ï¼
        cfg.task.train.resume_ckpt = None
        if cfg.gpu == 0:
            logger.info("[Transfer] cfg.resume_ckpt set to None. Optimizer will be reset.")

    # ... (åç»­ä»£ç ä¿æŒä¸å˜) ...
    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
    model = torch.nn.parallel.DistributedDataParallel(
        model, device_ids=[cfg.gpu], output_device=cfg.gpu, find_unused_parameters=True, broadcast_buffers=False)

    TrainLoop(
        cfg=cfg.task.train,
        model=model,
        diffusion=diffusion,
        dataloader=train_dataloader,
        device=device,
        save_dir=cfg.ckpt_dir,
        gpu=cfg.gpu,
        is_distributed=True,
    ).run_loop()

    if cfg.gpu == 0:
        Board().close()
        logger.info('[Train] ==> End training..')



if __name__ == '__main__':
    SEED = 2023
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    random.seed(SEED)
    np.random.seed(SEED)

    main()