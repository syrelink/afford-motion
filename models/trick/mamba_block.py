import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from timm.models.vision_transformer import Mlp
from timm.models.layers import DropPath
from mamba_ssm.ops.selective_scan_interface import selective_scan_fn
from einops import repeat


class _SelectiveStateSpace(nn.Module):
    """轻量版 Mamba Mixer，遵循官方 selective scan 接口。"""

    def __init__(self, d_model, d_state=16, d_conv=4, expand=2, dt_rank="auto", conv_bias=True, bias=False):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank

        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias)
        self.out_proj = nn.Linear(self.d_inner * 2, self.d_model, bias=bias)

        self.conv_x = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            groups=self.d_inner,
            kernel_size=d_conv,
            padding=d_conv - 1,
            bias=conv_bias,
        )

        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + self.d_state * 2, bias=False)
        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)

        A = repeat(torch.arange(1, self.d_state + 1, dtype=torch.float32), "n -> d n", d=self.d_inner)
        self.A_log = nn.Parameter(torch.log(A))
        self.D = nn.Parameter(torch.ones(self.d_inner))

    def forward(self, x):
        # x: [B, L, d_model]
        B, L, _ = x.shape
        xz = self.in_proj(x)
        x_inner, z_inner = xz.chunk(2, dim=-1)

        # depthwise conv
        x_inner = x_inner.transpose(1, 2)  # [B, d_inner, L]
        x_inner = self.conv_x(x_inner)[..., :L]
        x_inner = F.silu(x_inner).transpose(1, 2)  # [B, L, d_inner]

        params = self.x_proj(x_inner)
        dt, B_ssm, C_ssm = torch.split(params, [self.dt_rank, self.d_state, self.d_state], dim=-1)
        dt = self.dt_proj(dt).transpose(1, 2).contiguous()  # [B, d_inner, L]
        B_ssm = B_ssm.transpose(1, 2).contiguous()
        C_ssm = C_ssm.transpose(1, 2).contiguous()
        A = -torch.exp(self.A_log.float())

        y_ssm = selective_scan_fn(
            x_inner.transpose(1, 2).contiguous(),  # [B, d_inner, L]
            dt,
            A,
            B_ssm,
            C_ssm,
            self.D.float(),
            z=None,
            delta_bias=self.dt_proj.bias.float(),
            delta_softplus=True,
        )
        y_ssm = y_ssm.transpose(1, 2)  # [B, L, d_inner]

        z_inner = F.silu(z_inner)
        y = torch.cat([y_ssm, z_inner], dim=-1)
        return self.out_proj(y)


class MambaBlock(nn.Module):
    """带有 LayerNorm + FFN 的残差 Mamba 模块，兼容 padding_mask。"""

    def __init__(
        self,
        d_model,
        d_state=16,
        d_conv=4,
        expand=2,
        mlp_ratio=4.0,
        drop=0.0,
        drop_path=0.0,
    ):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.mixer = _SelectiveStateSpace(d_model, d_state=d_state, d_conv=d_conv, expand=expand)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = nn.LayerNorm(d_model)
        self.mlp = Mlp(in_features=d_model, hidden_features=int(d_model * mlp_ratio), act_layer=nn.GELU, drop=drop)

    def forward(self, x, padding_mask=None):
        # x: [B, L, C], padding_mask: [B, L]
        residual = x
        h = self.norm1(x)
        if padding_mask is not None:
            h = h.masked_fill(padding_mask.unsqueeze(-1), 0.0)
        h = self.mixer(h)
        if padding_mask is not None:
            h = h.masked_fill(padding_mask.unsqueeze(-1), 0.0)
        x = residual + self.drop_path(h)

        residual = x
        h = self.norm2(x)
        h = self.mlp(h)
        if padding_mask is not None:
            h = h.masked_fill(padding_mask.unsqueeze(-1), 0.0)
        x = residual + self.drop_path(h)
        return x


class BidirectionalMambaBlock(nn.Module):
    """
    双向 Mamba 模块：同时进行前向和后向扫描，通过线性层融合。
    适用于运动生成等非因果（Non-causal）序列任务。
    """

    def __init__(
            self,
            d_model,
            d_state=16,
            d_conv=4,
            expand=2,
            mlp_ratio=4.0,
            drop=0.0,
            drop_path=0.0,
    ):
        super().__init__()
        self.d_model = d_model

        # 1. Norm
        self.norm1 = nn.LayerNorm(d_model)

        # 2. 双向 Mixers (权重不共享，增加模型容量)
        self.mixer_forward = _SelectiveStateSpace(d_model, d_state=d_state, d_conv=d_conv, expand=expand)
        self.mixer_backward = _SelectiveStateSpace(d_model, d_state=d_state, d_conv=d_conv, expand=expand)

        # 3. 融合层：简单的线性投影将 2*d_model 降维回 d_model
        # 相比 MLP，这更节省参数且训练更稳定
        self.fusion_proj = nn.Linear(d_model * 2, d_model, bias=False)
        self.fusion_drop = nn.Dropout(drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()

        # 4. FFN 部分
        self.norm2 = nn.LayerNorm(d_model)
        self.mlp = Mlp(
            in_features=d_model,
            hidden_features=int(d_model * mlp_ratio),
            act_layer=nn.GELU,
            drop=drop
        )

    def forward(self, x, padding_mask=None):
        """
        Args:
            x: [B, L, C] 输入序列
            padding_mask: [B, L] True 表示是 padding (需要被 mask 掉)
        """
        residual = x
        h = self.norm1(x)

        # Apply Masking: 在进入 SSM 之前将 padding 区域置零
        # 这对于 SSM 很重要，虽然它有状态记忆，但输入为 0 能最大程度减少对后续状态的干扰
        if padding_mask is not None:
            # 确保 mask 维度匹配: [B, L, 1]
            h = h.masked_fill(padding_mask.unsqueeze(-1), 0.0)

        # --- Bidirectional Logic ---
        # 1. Forward Scan
        h_fwd = self.mixer_forward(h)

        # 2. Backward Scan (Flip -> Scan -> Flip Back)
        # flip(dims=[1]) 针对 [B, L, C] 的序列维度翻转
        h_bwd = torch.flip(h, dims=[1])
        h_bwd = self.mixer_backward(h_bwd)
        h_bwd = torch.flip(h_bwd, dims=[1])

        # 3. Fusion
        h_fused = torch.cat([h_fwd, h_bwd], dim=-1)  # [B, L, 2*C]
        h_fused = self.fusion_proj(h_fused)  # [B, L, C]
        h_fused = self.fusion_drop(h_fused)

        # Residual 1
        x = residual + self.drop_path(h_fused)

        # --- FFN Logic ---
        residual = x
        h = self.norm2(x)
        h = self.mlp(h)

        # Residual 2
        x = residual + self.drop_path(h)

        # 为了保险起见，输出时再次应用 mask，防止 padding 位置产生非零值噪声
        if padding_mask is not None:
            x = x.masked_fill(padding_mask.unsqueeze(-1), 0.0)

        return x


# 为了兼容性保留单向 Block（如果还想对比实验的话）
class MambaBlock(nn.Module):
    def __init__(self, d_model, d_state=16, d_conv=4, expand=2, mlp_ratio=4.0, drop=0.0, drop_path=0.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.mixer = _SelectiveStateSpace(d_model, d_state=d_state, d_conv=d_conv, expand=expand)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = nn.LayerNorm(d_model)
        self.mlp = Mlp(in_features=d_model, hidden_features=int(d_model * mlp_ratio), act_layer=nn.GELU, drop=drop)

    def forward(self, x, padding_mask=None):
        residual = x
        h = self.norm1(x)
        if padding_mask is not None:
            h = h.masked_fill(padding_mask.unsqueeze(-1), 0.0)
        h = self.mixer(h)
        x = residual + self.drop_path(h)
        residual = x
        h = self.norm2(x)
        h = self.mlp(h)
        x = residual + self.drop_path(h)
        return x