#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼º Perceiver çš„å…¼å®¹æ€§å’ŒåŠŸèƒ½
"""

import torch
from omegaconf import DictConfig
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from models.cdm import CDM
from models.trick.enhanced_perceiver import (
    EnhancedContactPerceiver,
    EnhancedTextEncoder,
    EnhancedPointEncoder,
    EnhancedCrossAttention,
    PhysicsConstraintLayer,
)


def test_enhanced_components():
    """æµ‹è¯•å¢å¼ºç»„ä»¶"""
    print("=" * 60)
    print("æµ‹è¯•å¢å¼ºç»„ä»¶")
    print("=" * 60)

    # æµ‹è¯•é…ç½®
    trans_dim = 256
    contact_dim = 2
    point_feat_dim = 256
    text_feat_dim = 512
    time_emb_dim = 256
    num_points = 1024

    # 1. æµ‹è¯• EnhancedTextEncoder
    print("\n1. æµ‹è¯• EnhancedTextEncoder...")
    text_encoder = EnhancedTextEncoder(text_feat_dim, trans_dim)
    text_emb = torch.randn(2, 1, text_feat_dim)
    scene_emb = torch.randn(2, num_points, trans_dim)
    text_out = text_encoder(text_emb, scene_emb)
    print(f"   è¾“å…¥: {text_emb.shape}, è¾“å‡º: {text_out.shape}")
    assert text_out.shape == (2, 1, trans_dim), "è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("   âœ… EnhancedTextEncoder æµ‹è¯•é€šè¿‡")

    # 2. æµ‹è¯• EnhancedPointEncoder
    print("\n2. æµ‹è¯• EnhancedPointEncoder...")
    point_encoder = EnhancedPointEncoder(contact_dim, point_feat_dim, trans_dim)
    x = torch.randn(2, num_points, contact_dim)
    point_feat = torch.randn(2, num_points, point_feat_dim)
    xyz = torch.randn(2, num_points, 3)
    point_out = point_encoder(x, point_feat, xyz)
    print(f"   è¾“å…¥: x={x.shape}, point_feat={point_feat.shape}, xyz={xyz.shape}")
    print(f"   è¾“å‡º: {point_out.shape}")
    assert point_out.shape == (2, num_points, trans_dim), "è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("   âœ… EnhancedPointEncoder æµ‹è¯•é€šè¿‡")

    # 3. æµ‹è¯• EnhancedCrossAttention
    print("\n3. æµ‹è¯• EnhancedCrossAttention...")
    cross_attn = EnhancedCrossAttention(trans_dim)
    query = torch.randn(2, 1, trans_dim)
    key_value = torch.randn(2, num_points, trans_dim)
    attn_out = cross_attn(query, key_value)
    print(f"   è¾“å…¥: query={query.shape}, key_value={key_value.shape}")
    print(f"   è¾“å‡º: {attn_out.shape}")
    assert attn_out.shape == (2, 1, trans_dim), "è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("   âœ… EnhancedCrossAttention æµ‹è¯•é€šè¿‡")

    # 4. æµ‹è¯• PhysicsConstraintLayer
    print("\n4. æµ‹è¯• PhysicsConstraintLayer...")
    physics_constraint = PhysicsConstraintLayer(trans_dim)
    features = torch.randn(2, num_points, trans_dim)
    xyz = torch.randn(2, num_points, 3)
    constrained = physics_constraint(features, xyz)
    print(f"   è¾“å…¥: features={features.shape}, xyz={xyz.shape}")
    print(f"   è¾“å‡º: features={constrained['features'].shape}")
    print(f"   æ¥è§¦æ¦‚ç‡: {constrained['contact_prob'].shape}")
    print(f"   æ¥è§¦ç±»å‹: {constrained['contact_type'].shape}")
    assert constrained['features'].shape == (2, num_points, trans_dim), "è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    assert constrained['contact_prob'].shape == (2, num_points, 1), "æ¥è§¦æ¦‚ç‡å½¢çŠ¶é”™è¯¯"
    assert constrained['contact_type'].shape == (2, num_points, 4), "æ¥è§¦ç±»å‹å½¢çŠ¶é”™è¯¯"
    print("   âœ… PhysicsConstraintLayer æµ‹è¯•é€šè¿‡")

    # 5. æµ‹è¯• EnhancedContactPerceiver
    print("\n5. æµ‹è¯• EnhancedContactPerceiver...")
    arch_cfg = DictConfig({
        'trans_dim': trans_dim,
        'last_dim': 256,
        'num_neighbors': 16,
        'dropout': 0.1,
    })
    perceiver = EnhancedContactPerceiver(
        arch_cfg, contact_dim, point_feat_dim, text_feat_dim, time_emb_dim
    )
    x = torch.randn(2, num_points, contact_dim)
    point_feat = torch.randn(2, num_points, point_feat_dim)
    language_feat = torch.randn(2, 1, text_feat_dim)
    time_embedding = torch.randn(2, 1, time_emb_dim)
    c_pc_xyz = torch.randn(2, num_points, 3)
    perceiver_out = perceiver(x, point_feat, language_feat, time_embedding, c_pc_xyz=c_pc_xyz)
    print(f"   è¾“å…¥: x={x.shape}, point_feat={point_feat.shape}")
    print(f"   è¾“å‡º: {perceiver_out.shape}")
    assert perceiver_out.shape == (2, num_points, arch_cfg.last_dim), "è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("   âœ… EnhancedContactPerceiver æµ‹è¯•é€šè¿‡")

    print("\n" + "=" * 60)
    print("æ‰€æœ‰å¢å¼ºç»„ä»¶æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


def test_cdm_integration():
    """æµ‹è¯• CDM é›†æˆ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• CDM é›†æˆ")
    print("=" * 60)

    # æµ‹è¯•é…ç½®
    cfg = DictConfig({
        'arch': 'EnhancedPerceiver',
        'arch_enhanced_perceiver': {
            'trans_dim': 256,
            'last_dim': 256,
            'num_neighbors': 16,
            'dropout': 0.1,
        },
        'data_repr': 'contact_map',
        'input_feats': 2,
        'time_emb_dim': 256,
        'text_model': {
            'version': 'clip-ViT-B/32',
            'max_length': 77,
        },
        'scene_model': {
            'use_scene_model': True,
            'name': 'pointtransformer',
            'point_feat_dim': 256,
            'num_points': 1024,
            'pretrained_weight': None,
            'freeze': True,
        },
    })

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»º CDM æ¨¡å‹
    print("\nåˆ›å»º CDM æ¨¡å‹...")
    model = CDM(cfg, device=device).to(device)

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"\næ¨¡å‹æ¶æ„: {cfg.arch}")
    print(f"æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
    batch_size = 2
    num_points = 1024

    x = torch.randn(batch_size, num_points, 2).to(device)
    timesteps = torch.randint(0, 1000, (batch_size,)).to(device)
    c_text = ["a person sitting on a chair"] * batch_size
    c_pc_xyz = torch.randn(batch_size, num_points, 3).to(device)
    c_pc_feat = torch.randn(batch_size, num_points, 256).to(device)

    with torch.no_grad():
        output = model(
            x=x,
            timesteps=timesteps,
            c_text=c_text,
            c_pc_xyz=c_pc_xyz,
            c_pc_feat=c_pc_feat,
        )

    print(f"è¾“å…¥å½¢çŠ¶: x={x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    assert output.shape == (batch_size, num_points, 2), "è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("   âœ… CDM é›†æˆæµ‹è¯•é€šè¿‡")

    print("\n" + "=" * 60)
    print("CDM é›†æˆæµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§ï¼ˆåŸå§‹ Perceiverï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å‘åå…¼å®¹æ€§ï¼ˆåŸå§‹ Perceiverï¼‰")
    print("=" * 60)

    # æµ‹è¯•åŸå§‹ Perceiver
    cfg = DictConfig({
        'arch': 'Perceiver',
        'arch_perceiver': {
            'encoder_q_input_channels': 256,
            'encoder_kv_input_channels': 256,
            'encoder_num_heads': 8,
            'encoder_widening_factor': 2,
            'encoder_dropout': 0.1,
            'encoder_residual_dropout': 0.1,
            'encoder_self_attn_num_layers': 1,
            'decoder_q_input_channels': 256,
            'decoder_kv_input_channels': 256,
            'decoder_num_heads': 8,
            'decoder_widening_factor': 2,
            'decoder_dropout': 0.1,
            'decoder_residual_dropout': 0.1,
            'point_pos_emb': False,
            'last_dim': 256,
        },
        'data_repr': 'contact_map',
        'input_feats': 2,
        'time_emb_dim': 256,
        'text_model': {
            'version': 'clip-ViT-B/32',
            'max_length': 77,
        },
        'scene_model': {
            'use_scene_model': True,
            'name': 'pointtransformer',
            'point_feat_dim': 256,
            'num_points': 1024,
            'pretrained_weight': None,
            'freeze': True,
        },
    })

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ›å»º CDM æ¨¡å‹ï¼ˆåŸå§‹ Perceiverï¼‰
    print("\nåˆ›å»º CDM æ¨¡å‹ï¼ˆåŸå§‹ Perceiverï¼‰...")
    model = CDM(cfg, device=device).to(device)

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
    batch_size = 2
    num_points = 1024

    x = torch.randn(batch_size, num_points, 2).to(device)
    timesteps = torch.randint(0, 1000, (batch_size,)).to(device)
    c_text = ["a person sitting on a chair"] * batch_size
    c_pc_xyz = torch.randn(batch_size, num_points, 3).to(device)
    c_pc_feat = torch.randn(batch_size, num_points, 256).to(device)

    with torch.no_grad():
        output = model(
            x=x,
            timesteps=timesteps,
            c_text=c_text,
            c_pc_xyz=c_pc_xyz,
            c_pc_feat=c_pc_feat,
        )

    print(f"è¾“å…¥å½¢çŠ¶: x={x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    assert output.shape == (batch_size, num_points, 2), "è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("   âœ… åŸå§‹ Perceiver æµ‹è¯•é€šè¿‡")

    print("\n" + "=" * 60)
    print("å‘åå…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


if __name__ == "__main__":
    try:
        test_enhanced_components()
        test_cdm_integration()
        test_backward_compatibility()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼º Perceiver å·²æˆåŠŸé›†æˆåˆ° CDM ä¸­ï¼")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
